{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started \n",
    "\n",
    "Make sure you have installed all the libraries in the import section below.  If you get any errors, look up the documentation for each library for help.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:48.111237",
     "start_time": "2017-07-17T11:35:39.783333Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geoplot as gplt\n",
    "import gdelt\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tzwhere import tzwhere \n",
    "import pytz\n",
    "\n",
    "tz1 = tzwhere.tzwhere(forceTZ=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up `gdeltPyR`\n",
    "\n",
    "It's easy to set up `gdeltPyR`.  This single line gets us ready to query.  See the github project page for details on accessing other tables and setting other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:49.365530",
     "start_time": "2017-07-17T11:35:48.113822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "marawi = pickle.load(open('/Users/linwood/Documents/marawievents.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:49.413008",
     "start_time": "2017-07-17T11:35:49.367793Z"
    }
   },
   "outputs": [],
   "source": [
    "gd = gdelt.gdelt()\n",
    "\n",
    "# %time marawi = gd.Search(['2017 May 23'],normcols=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:49.514531",
     "start_time": "2017-07-17T11:35:49.415398Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def striptimen(x):\n",
    "    \"\"\"Strip time from numpy array or list of dates that are integers\"\"\"\n",
    "    date = str(int(x))\n",
    "    n = np.datetime64(\"{}-{}-{}T{}:{}:{}\".format(date[:4],date[4:6],date[6:8],date[8:10],date[10:12],date[12:]))\n",
    "    return n\n",
    "\n",
    "def timeget(x):\n",
    "    '''convert to datetime object with UTC time tag'''\n",
    "    \n",
    "    try:\n",
    "        now_aware = pytz.utc.localize(x[2].to_pydatetime())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get the timezone string representation using lat/lon pair\n",
    "    try:\n",
    "        timezone_str=tz1.tzNameAt(x[0],x[1],forceTZ=True)\n",
    "        \n",
    "            # get the time offset\n",
    "        timezone = pytz.timezone(timezone_str)\n",
    "\n",
    "        # convert UTC to calculated local time\n",
    "        aware = now_aware.astimezone(timezone)\n",
    "        return aware\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "# vectorize our two functions\n",
    "vect = np.vectorize(striptimen)\n",
    "vect2=np.vectorize(timeget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:50.464739",
     "start_time": "2017-07-17T11:35:49.516946Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = vect(marawi.dateadded.values)\n",
    "marawi = marawi.assign(dates=dates)\n",
    "marawi.set_index(dates,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.464915",
     "start_time": "2017-07-17T11:35:50.468552Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datetz = [timeget(l) for l in marawi[['actiongeolat','actiongeolong','dates']][marawi[['actiongeolat','actiongeolong','dates']].notnull()==True].values.tolist()]\n",
    "marawi=marawi.assign(datezone=datetz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.562630",
     "start_time": "2017-07-17T11:36:18.467279Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maute2= marawi[(marawi.actiongeofeatureid=='-2438515') \\\n",
    "               & (marawi.eventrootcode=='19')]\n",
    "\n",
    "maute2.sort_values('datezone')[['datezone','sourceurl']].drop_duplicates('sourceurl').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.606198",
     "start_time": "2017-07-17T11:36:18.565338Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternative to strip all known website domains and add to regex\n",
    "\n",
    "# endings = pd.read_html('https://iwantmyname.com/domains/domain-name-registration-list-of-extensions')[0]\n",
    "# endings.columns = ['Domain extension','USD per year','Description']\n",
    "# endings = endings.assign(doms=endings['Domain extension'].apply(lambda x: (\"\\\\\"+ x.split(' ')[0])))\n",
    "# endingslist = endings['doms'].values.tolist()\n",
    "# endingslist.append('\\.IE')\n",
    "# endingsString = \"|\".join(endingslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.694396",
     "start_time": "2017-07-17T11:36:18.654706Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# known domain regex\n",
    "# r = re.compile('()({})'.format(endingsString),flags = re.IGNORECASE)\n",
    "\n",
    "# lazy meta-character regex; more elegant\n",
    "s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T12:53:05.990933",
     "start_time": "2017-07-17T12:53:05.911164Z"
    }
   },
   "outputs": [],
   "source": [
    "frame = maute2\n",
    "\n",
    "frame = frame.drop_duplicates(['sourceurl'])\n",
    "\n",
    "frame=frame.assign(provider=frame.sourceurl.\\\n",
    "      apply(lambda x: s.search(x).group() if s.search(x) else np.nan))\n",
    "\n",
    "groups = frame.groupby(['provider']).size().sort_values(ascending=False).reset_index()\n",
    "groups.columns = ['provider','count']\n",
    "groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.848151",
     "start_time": "2017-07-17T11:36:18.802981Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame2 = frame.copy()[frame.provider.notnull()==True].drop_duplicates('sourceurl')[['provider','sourceurl','dates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.893411",
     "start_time": "2017-07-17T11:36:18.850670Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame2 = frame2.assign(dates=frame2['dates'].apply(lambda x: (x.to_pydatetime().timestamp())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.955683",
     "start_time": "2017-07-17T11:36:18.895582Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp = frame2.groupby('provider').filter(lambda x: len(x)>=3).groupby('provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:19.020003",
     "start_time": "2017-07-17T11:36:18.959159Z"
    }
   },
   "outputs": [],
   "source": [
    "final = grp.agg([np.mean,np.max,np.min]).sortlevel('mean',ascending=False)\n",
    "newfinal = pd.DataFrame(final['dates']['mean'].apply(lambda x:datetime.datetime.fromtimestamp(int(x))).sort_values(ascending=True)).reset_index().set_index('mean',drop=False)\n",
    "newfinal = newfinal.tz_localize('UTC')\n",
    "newfinal = newfinal.tz_convert('Asia/Manila')\n",
    "newfinal.columns = ['provider','UTC Time']\n",
    "newfinal.index.name='Philippines Time'\n",
    "# print(newfinal.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T04:07:15.444755",
     "start_time": "2017-07-17T04:07:14.531401Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "mpl.style.use('economist')\n",
    "timeseries= pd.concat([marawi.resample('H')['sourceurl'].count(),maute2.resample('H')['sourceurl'].count()]\n",
    "         ,axis=1)\n",
    "timeseries.fillna(0,inplace=True)\n",
    "timeseries.columns = ['Total Events','Marawi Violent Events Only']\n",
    "timeseries = timeseries\\\n",
    ".assign(Normalized=(timeseries['Marawi Violent Events Only']/timeseries['Total Events'])*100)\n",
    "f,ax = plt.subplots(figsize=(13,7))\n",
    "ax = timeseries.Normalized\\\n",
    ".ewm(adjust=True,ignore_na=True,min_periods=5,span=12).mean()\\\n",
    ".plot(color=\"#C10534\",label='Exponentially Weighted Count', linewidth=4.8)\n",
    "ax.set_title('Hourly Count of Violent Events in Marawi',fontsize=28)\n",
    "for label in ax.get_xticklabels():\n",
    "      label.set_fontsize(16)\n",
    "ax.set_xlabel('Hour of the Day', fontsize=20)\n",
    "ax.set_ylabel('Percentage of Hourly Total',fontsize='15')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/img/countGraphic.png')\n",
    "# pd.date_range(start='2017 23 May 00:00:00', end='2017 23 May 23:59:59',freq='1H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:38:10.816139",
     "start_time": "2017-07-17T11:38:10.728308Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maute2= marawi[(marawi.actiongeofeatureid=='-2438515') \\\n",
    "               & (marawi.eventrootcode=='19')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:45:39.629080",
     "start_time": "2017-07-17T11:45:39.586036Z"
    }
   },
   "outputs": [],
   "source": [
    "maute2.sourceurl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:54:59.039834",
     "start_time": "2017-07-17T11:54:59.000107Z"
    }
   },
   "outputs": [],
   "source": [
    "marawi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:44:04.288000",
     "start_time": "2017-07-17T11:44:04.244554Z"
    }
   },
   "outputs": [],
   "source": [
    "maute2.drop_duplicates('sourceurl',keep='first').sort_values('dates',ascending=True)['sourceurl'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T19:16:25.445508",
     "start_time": "2017-07-16T19:16:25.350646Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Linwood Creekmore\n",
    "# Email: valinvescap@gmail.com\n",
    "# Description:  Python script to pull content from a website (works on news stories).\n",
    "\n",
    "###################################\n",
    "# Standard Library imports\n",
    "###################################\n",
    "\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "###################################\n",
    "# Third party imports\n",
    "###################################\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "done ={}\n",
    "def textgetter(url):\n",
    "    \"\"\"Scrapes web news and returns the content\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    url : str\n",
    "        Address to news report\n",
    "        \n",
    "    newstext: str\n",
    "        Returns all text in the \"p\" tag.  This usually is the content of the news story.\n",
    "    \"\"\"\n",
    "    global done\n",
    "    \n",
    "    # regex for url check\n",
    "    s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')\n",
    "    \n",
    "    # check that its an url\n",
    "    if s.search(url):\n",
    "        if url in done.keys():\n",
    "            return done[url]\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            r  = requests.get(url)\n",
    "            if r.status_code != 200:\n",
    "                done[url]=\"Unable to reach website.\"\n",
    "                return {url:\"Unable to reach website.\"}\n",
    "\n",
    "            data = r.content\n",
    "\n",
    "            soup = BeautifulSoup(data,'html.parser')\n",
    "\n",
    "            newstext = \" \".join([l.text for l in soup.find_all('p')])\n",
    "            done[url]=newstext\n",
    "            del r\n",
    "            if len(newstext)>200:\n",
    "                return {url:newstext}\n",
    "            else:\n",
    "                newstext = \" \".join([l.text for l in soup.find_all('div',class_='field-item even')])\n",
    "                done[url]=newstext\n",
    "                if len(newstext)>200:\n",
    "                    return {url:newstext}\n",
    "                else:\n",
    "                    return {url: \"No text returned\"}\n",
    "    else:\n",
    "        return {url:\"This is not a proper url.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T12:48:27.026650",
     "start_time": "2017-07-17T12:48:26.884342Z"
    }
   },
   "outputs": [],
   "source": [
    "# Author: Linwood Creekmore\n",
    "# Email: valinvescap@gmail.com\n",
    "# Description:  Python script to pull content from a website (works on news stories).\n",
    "\n",
    "###################################\n",
    "# Standard Library imports\n",
    "###################################\n",
    "\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "###################################\n",
    "# Third party imports\n",
    "###################################\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# placehoder to store completed urls; like caching\n",
    "done ={}\n",
    "def textgetter(url):\n",
    "    \"\"\"Scrapes web news and returns the content\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    url : str\n",
    "        Address to news report\n",
    "        \n",
    "    newstext: str\n",
    "        Returns all text in the \"p\" tag.  This usually is the content of the news story.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    dict: key/value\n",
    "        Returns dictionary with key = url and value = content/message\n",
    "    \"\"\"\n",
    "    global done\n",
    "    \n",
    "    # regex for url check\n",
    "    s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')\n",
    "    \n",
    "    # check that its an url\n",
    "    if s.search(url):\n",
    "        if url in done.keys():\n",
    "            return done[url]\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            # Make the call to the new story\n",
    "            r  = requests.get(url)\n",
    "            # check for a good response; return message otherwise\n",
    "            if r.status_code != 200:\n",
    "                done[url]=\"Unable to reach website.\"\n",
    "                return {url:\"Unable to reach website.\"}\n",
    "            # store bytes of message in variable\n",
    "            data = r.content\n",
    "            # parse HTML \n",
    "            soup = BeautifulSoup(data,'html.parser')\n",
    "            # strip paragraphs from HTML and join into a string\n",
    "            newstext = \" \".join([l.text for l in soup.find_all('p')])\n",
    "            # add to done dictionary to prevent duplication\n",
    "            done[url]=newstext\n",
    "            # delete the response; save memory\n",
    "            del r\n",
    "            # check if return is longer than average sentence\n",
    "            if len(newstext)>200:\n",
    "                return {url:newstext}\n",
    "            else:\n",
    "                # check for another place where text is stored\n",
    "                newstext = \" \".join([l.text for l in soup.find_all('div',class_='field-item even')])\n",
    "                done[url]=newstext\n",
    "                # check for length; must be longer than a sentence\n",
    "                if len(newstext)>200:\n",
    "                    return {url:newstext}\n",
    "                else:\n",
    "                    # if all fails, return message\n",
    "                    return {url: \"No text returned\"}\n",
    "    else:\n",
    "        # if we don't pass very first test; not a url\n",
    "        return {url:\"This is not a proper url.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T12:47:12.249766",
     "start_time": "2017-07-17T12:47:01.969779Z"
    }
   },
   "outputs": [],
   "source": [
    "urls = frame2['sourceurl'].unique()\n",
    "textgetter(urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T19:16:26.008774",
     "start_time": "2017-07-16T19:16:26.003795Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d=maute2.sourceurl.drop_duplicates().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T19:25:10.518879",
     "start_time": "2017-07-16T19:24:30.086028Z"
    }
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "e = ProcessPoolExecutor()\n",
    "%time results = list(e.map(textgetter,urls))\n",
    "# %time results2 = np.array(list(e.map(textgetter,d)))\n",
    "# done = {}\n",
    "# %time results3 = marawi.sourceurl.drop_duplicates()[1240:1245].apply(textgetter).values\n",
    "# %time results3 = marawi.sourceurl.drop_duplicates()[1240:1245].apply(textgetter).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T19:22:54.658088",
     "start_time": "2017-07-16T19:22:54.640348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sers = []\n",
    "\n",
    "for l in results:\n",
    "    ul = list(l.keys())[0]\n",
    "    content = l[ul]\n",
    "    sers.append(pd.Series({'url':ul,'content':content}))\n",
    "connie = pd.concat(sers,axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T22:02:30.163327",
     "start_time": "2017-07-16T22:02:30.107568Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maute2 = maute2.merge(connie,left_on='sourceurl',right_on='url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T22:15:41.203896",
     "start_time": "2017-07-16T22:15:41.156665Z"
    }
   },
   "outputs": [],
   "source": [
    "for l in maute2.drop_duplicates('sourceurl').sort_values('dates',ascending=True).content.values:\n",
    "    print (l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T18:40:34.989431",
     "start_time": "2017-07-16T18:40:34.967258Z"
    }
   },
   "outputs": [],
   "source": [
    "rellie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T18:30:40.373069",
     "start_time": "2017-07-16T18:30:01.167376Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rellie = marawi.sourceurl.drop_duplicates()[20:30].apply(textgetter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T15:39:58.687638",
     "start_time": "2017-07-16T15:39:58.641759Z"
    }
   },
   "outputs": [],
   "source": [
    "b = pd.concat([d,maute2.sourceurl.drop_duplicates()],axis=1)\n",
    "b.columns= ['content','url']\n",
    "b[b.content==\"Unable to reach website.\"]['url'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T15:24:20.939638",
     "start_time": "2017-07-16T15:24:20.894514Z"
    }
   },
   "outputs": [],
   "source": [
    "len(done.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T15:09:39.061392",
     "start_time": "2017-07-16T15:09:39.000560Z"
    }
   },
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T19:10:13.418163",
     "start_time": "2017-07-16T19:10:11.035773Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ur = 'http://www.philstar.com/nation/2017/05/23/1702882/marawi-residents-told-stay-home-firefights-continue'\n",
    "page = requests.get(ur)\n",
    "soup = BeautifulSoup(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T19:11:33.959959",
     "start_time": "2017-07-16T19:11:33.931075Z"
    }
   },
   "outputs": [],
   "source": [
    "\" \".join([l.text for l in soup.find_all('div',class_='field-item even')])                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T15:18:50.216522",
     "start_time": "2017-07-16T15:18:50.163247Z"
    }
   },
   "outputs": [],
   "source": [
    "\" \".join([l.text for l in soup.find_all('div',class_='field-item even')])\n",
    "# soup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-15T10:26:57.140738",
     "start_time": "2017-07-15T10:26:57.127993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mautesub[['sourceurl','dateadded','datezone']].drop_duplicates('sourceurl').sort_values('datezone',ascending=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-16T13:52:51.606105",
     "start_time": "2017-07-16T13:52:51.563768Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(maute2.sort_values('datezone')[['datezone','sourceurl']].drop_duplicates('sourceurl').head().to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-14T04:28:37.779449",
     "start_time": "2017-07-14T04:28:35.764543Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holder = margkg['date'][margkg['date'].notnull()==True].index\n",
    "margkg=margkg.assign(datefix=margkg['date'])\n",
    "margkg['datefix'].loc[holder]=vect(margkg['date'].values[holder])\n",
    "print(margkg['datefix'][margkg['datefix'].notnull()==False])\n",
    "margkg['datefix']=margkg['datefix'].fillna(method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = datframe.sample(samplenum)\n",
    "ax = gplt.polyplot(polyframe,projection=gcrs.PlateCarree(),figsize=(20,12))\n",
    "gplt.kdeplot(samp,ax=ax,shade=True,shade_lowest=False,projection=gcrs.TransverseMercator())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:geo]",
   "language": "python",
   "name": "conda-env-geo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
