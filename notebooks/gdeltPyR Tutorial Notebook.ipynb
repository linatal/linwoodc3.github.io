{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Author: Linwood Creekmore<br>\n",
    "Email: valinvescap@gmail.com</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started \n",
    "\n",
    "Make sure you have installed all the libraries in the import section below.  If you get any errors, look up the documentation for each library for help.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T13:11:48.068889",
     "start_time": "2017-10-23T13:11:29.080232Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pytz\n",
    "import gdelt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geoplot as gplt\n",
    "from tzwhere import tzwhere \n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tz1 = tzwhere.tzwhere(forceTZ=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up `gdeltPyR`\n",
    "\n",
    "It's easy to set up `gdeltPyR`.  This single line gets us ready to query.  See the [github project page](https://github.com/linwoodc3/gdeltPyR/blob/master/README.md) for details on accessing other tables and setting other parameters. Then, we just pass in a date to pull the data.  It's really that simple.  The only concern, is memory.  Pulling multiple days of GDELT can consume lots of memory.  Make a workflow to pull and write the disc if you have issues.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:49.413008",
     "start_time": "2017-07-17T11:35:49.367793Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gd = gdelt.gdelt()\n",
    "\n",
    "%time vegas = gd.Search(['Oct 1 2017','Oct 2 2017'],normcols=True,coverage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time format transformations\n",
    "\n",
    "These custom function handle time transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:49.514531",
     "start_time": "2017-07-17T11:35:49.415398Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def striptimen(x):\n",
    "    \"\"\"Strip time from numpy array or list of dates that are integers\"\"\"\n",
    "    date = str(int(x))\n",
    "    n = np.datetime64(\"{}-{}-{}T{}:{}:{}\".format(date[:4],date[4:6],date[6:8],date[8:10],date[10:12],date[12:]))\n",
    "    return n\n",
    "\n",
    "def timeget(x):\n",
    "    '''convert to datetime object with UTC time tag'''\n",
    "    \n",
    "    try:\n",
    "        now_aware = pytz.utc.localize(x[2].to_pydatetime())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # get the timezone string representation using lat/lon pair\n",
    "    try:\n",
    "        timezone_str=tz1.tzNameAt(x[0],x[1],forceTZ=True)\n",
    "        \n",
    "            # get the time offset\n",
    "        timezone = pytz.timezone(timezone_str)\n",
    "\n",
    "        # convert UTC to calculated local time\n",
    "        aware = now_aware.astimezone(timezone)\n",
    "        return aware\n",
    "    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "# vectorize our two functions\n",
    "vect = np.vectorize(striptimen)\n",
    "vect2=np.vectorize(timeget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the functions to create a datetime object column (`dates`) and a timezone aware column (`datezone`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:35:50.464739",
     "start_time": "2017-07-17T11:35:49.516946Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vectorize our function\n",
    "vect = np.vectorize(striptimen)\n",
    "\n",
    "\n",
    "# use custom functions to build time enabled columns of dates and zone\n",
    "vegastimed = (vegas.assign(\n",
    "                dates=vect(vegas.dateadded.values)).assign(\n",
    "                zone=list(timeget(k) for k in vegas.assign(\n",
    "                dates=vect(vegas.dateadded.values))\\\n",
    "                 [['actiongeolat','actiongeolong','dates']].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering to a city and specific CAMEO Code\n",
    "\n",
    "I return data in `pandas dataframes` to leverage the power of pandas data manipulation.  Now we filter our data on the two target fields; actiongeofeatureid and eventrootcode.  To learn more about the columns, see [this page with descriptions for each header](https://github.com/linwoodc3/gdelt2HeaderRows/blob/master/schema_csvs/GDELT_2.0_Events_Column_Labels_Header_Row_Sep2016.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.562630",
     "start_time": "2017-07-17T11:36:18.467279Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter to data in Las Vegas and about violence/fighting/mass murder only\n",
    "vegastimedfil=(vegastimed[\n",
    "                        ((vegas.eventrootcode=='19') | \n",
    "                        (vegas.eventrootcode=='20') | \n",
    "                        (vegas.eventrootcode=='18')) & \n",
    "                         (vegas.actiongeofeatureid=='847388')])\\\n",
    "                                    .drop_duplicates('sourceurl') \n",
    "print(vegastimedfil.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stripping out unique news providers\n",
    "\n",
    "This regex extracts baseurls from the `sourceurl` column.  These extractions allow us to analyze the contributions of unique providers in GDELT `events` data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:18.694396",
     "start_time": "2017-07-17T11:36:18.654706Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lazy meta-character regex; more elegant\n",
    "s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Chronological List\n",
    "\n",
    "If you want to see a chronological list, you'll need to time enable your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the chronological news stories and show the first few rows\n",
    "print(vegastimedfil.set_index('zone')[['dates','sourceurl']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To time enable the entire dataset, it's a fairly simple task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example of converting to Los Angeles time. \n",
    "vegastimed.set_index(\n",
    "    vegastimed.dates.astype('datetime64[ns]')\n",
    "                    ).tz_localize(\n",
    "                                'UTC'\n",
    "                                ).tz_convert(\n",
    "                                            'America/Los_Angeles'\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Counting Who Produced the Most\n",
    "\n",
    "We use pandas to find the provider with the most unique content.  One drawback of GDELT, is repeated URLs.  But, in the pandas ecosystem, removing duplicates is easy.  We extract provider baseurls, remove duplicates, and count the number of articles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T12:53:05.990933",
     "start_time": "2017-07-17T12:53:05.911164Z"
    }
   },
   "outputs": [],
   "source": [
    "# regex to strip a url from a string; should work on any url (let me know if it doesn't)\n",
    "s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')\n",
    "\n",
    "# apply regex to each url; strip provider; assign as new column\n",
    "print(vegastimedfil.assign(provider=vegastimedfil.sourceurl.\\\n",
    "      apply(lambda x: s.search(x).group() if s.search(x) else np.nan))\\\n",
    ".groupby(['provider']).size().sort_values(ascending=False).reset_index().rename(columns={0:\"count\"}).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many unique news providers?\n",
    "\n",
    "This next block uses regex to strip the base URL from each record.  Then, you just use the `pandas.Series.unique()` method to get a total count of providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chained operation to return shape\n",
    "vegastimedfil.assign(provider=vegastimedfil.sourceurl.\\\n",
    "      apply(lambda x: s.search(x).group() if \\\n",
    "      s.search(x) else np.nan))['provider']\\\n",
    ".value_counts().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how many providers we have producing, it would be a good idea to understand the distribution of production.  Or, we want to see how many articles each provider published. We use a distribution and cumulative distribution plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make plot canvas\n",
    "f,ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "# set title\n",
    "plt.title('Distributions of Las Vegas Active Shooter News Production')\n",
    "\n",
    "# ckernel density plot\n",
    "sns.kdeplot(vegastimedfil.assign(provider=vegastimedfil.sourceurl.\\\n",
    "      apply(lambda x: s.search(x).group() if s.search(x) else np.nan))['provider']\\\n",
    ".value_counts(),bw=0.4,shade=True,label='No. of articles written',ax=ax)\n",
    "\n",
    "# cumulative distribution plot\n",
    "sns.kdeplot(vegastimedfil.assign(provider=vegastimedfil.sourceurl.\\\n",
    "      apply(lambda x: s.search(x).group() if s.search(x) else np.nan))['provider']\\\n",
    ".value_counts(),bw=0.4,shade=True,label='Cumulative',cumulative=True,ax=ax)\n",
    "\n",
    "# show it\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series: Calculating the volumetric change\n",
    "\n",
    "Next, we use the exponentially weighted moving average to see the change in production.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timeseries = pd.concat([vegastimed.set_index(vegastimed.dates.astype('datetime64[ns]')).tz_localize('UTC').tz_convert('America/Los_Angeles').resample('15T')['sourceurl'].count(),vegastimedfil.set_index('zone').resample('15T')['sourceurl'].count()]\n",
    "         ,axis=1)\n",
    "\n",
    "# file empty event counts with zero\n",
    "timeseries.fillna(0,inplace=True)\n",
    "\n",
    "# rename columns\n",
    "timeseries.columns = ['Total Events','Las Vegas Events Only']\n",
    "\n",
    "# combine\n",
    "timeseries = timeseries.assign(Normalized=(timeseries['Las Vegas Events Only']/timeseries['Total Events'])*100)\n",
    "\n",
    "# make the plot\n",
    "f,ax = plt.subplots(figsize=(13,7))\n",
    "ax = timeseries.Normalized.ewm(adjust=True,ignore_na=True,min_periods=10,span=20).mean().plot(color=\"#C10534\",label='Exponentially Weighted Count')\n",
    "ax.set_title('Reports of Violent Events Per 15 Minutes in Vegas',fontsize=28)\n",
    "for label in ax.get_xticklabels():\n",
    "      label.set_fontsize(16)\n",
    "ax.set_xlabel('Hour of the Day', fontsize=20)\n",
    "ax.set_ylabel('Percentage of Hourly Total',fontsize='15')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Who Produced the \"Fastest\"\n",
    "\n",
    "This block of code finds the news provider who produced reports faster \"on average\".  We convert the date of each article to epoch time, average across providers, and compare.  Again, `pandas` makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T11:36:19.020003",
     "start_time": "2017-07-17T11:36:18.959159Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# complex, chained operations to perform all steps listed above\n",
    "\n",
    "print((((vegastimedfil.reset_index().assign(provider=vegastimedfil.reset_index().sourceurl.\\\n",
    "      apply(lambda x: s.search(x).group() if s.search(x) else np.nan),\\\n",
    "                                      epochzone=vegastimedfil.set_index('dates')\\\n",
    "                                      .reset_index()['dates']\\\n",
    ".apply(lambda x: (x.to_pydatetime().timestamp()))).groupby('provider')\\\n",
    ".filter(lambda x: len(x)>=10).groupby('provider').agg([np.mean,np.max,np.min,np.median])\\\n",
    ".sort_index(level='median',ascending=False)['epochzone']['median'])\\\n",
    "  .apply(lambda x:datetime.datetime.fromtimestamp(int(x)))\\\n",
    ".sort_values(ascending=True)).reset_index()\\\n",
    " .set_index('median',drop=False)).tz_localize('UTC')\\\n",
    ".tz_convert('America/Los_Angeles'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Content\n",
    "\n",
    "This code gets the content (or tries to) at the end of each GDELT sourceurl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T13:08:48.513185",
     "start_time": "2017-07-17T13:08:47.941535Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Linwood Creekmore\n",
    "# Email: valinvescap@gmail.com\n",
    "# Description:  Python script to pull content from a website (works on news stories).\n",
    "\n",
    "# Notes\n",
    "\"\"\"\n",
    "23 Oct 2017: updated to include readability based on PyCon talk: https://github.com/DistrictDataLabs/PyCon2016/blob/master/notebooks/tutorial/Working%20with%20Text%20Corpora.ipynb\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "###################################\n",
    "# Standard Library imports\n",
    "###################################\n",
    "\n",
    "import re\n",
    "from io import BytesIO\n",
    "\n",
    "###################################\n",
    "# Third party imports\n",
    "###################################\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from readability.readability import Document as Paper\n",
    "\n",
    "# placeholder dictionary to keep track of what's been completed\n",
    "done ={}\n",
    "def textgetter(url):\n",
    "    \"\"\"Scrapes web news and returns the content\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    url : str\n",
    "        Address to news report\n",
    "        \n",
    "    newstext: str\n",
    "        Returns all text in the \"p\" tag.  This usually is the content of the news story.\n",
    "    \"\"\"\n",
    "    global done\n",
    "    TAGS = [\n",
    "        'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li'\n",
    "    ]\n",
    "    \n",
    "    # regex for url check\n",
    "    s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')\n",
    "    answer = {}\n",
    "    # check that its an url\n",
    "    if s.search(url):\n",
    "        if url in done.keys():\n",
    "            return done[url]\n",
    "            pass\n",
    "        else:\n",
    "\n",
    "            r  = requests.get(url)\n",
    "            if r.status_code != 200:\n",
    "                done[url]=\"Unable to reach website.\"\n",
    "                answer['base']=s.search(url).group()\n",
    "                answer['url']=url\n",
    "                answer['text']=\"Unable to reach website.\"\n",
    "                answer['title']=''\n",
    "                yield answer\n",
    "                \n",
    "            doc = Paper(r.content)\n",
    "            data = doc.summary()\n",
    "            title = doc.title()\n",
    "\n",
    "            soup = BeautifulSoup(data,'lxml')\n",
    "\n",
    "            newstext = \" \".join([l.text for l in soup.find_all(TAGS)]) \n",
    "            \n",
    "            del r,data\n",
    "            if len(newstext)>200:\n",
    "                answer['base']=s.search(url).group()\n",
    "                answer['text']=newstext\n",
    "                answer['url']=url\n",
    "                answer['title']=title\n",
    "                yield answer\n",
    "            else:\n",
    "                newstext = \" \".join([l.text for l in soup.find_all('div',class_='field-item even')])\n",
    "                done[url]=newstext\n",
    "                if len(newstext)>200:\n",
    "                    answer['url']=url\n",
    "                    answer['base']=s.search(url).group()\n",
    "                    answer['text']=newstext\n",
    "                    answer['title']=\"\"\n",
    "                    yield answer\n",
    "                else:\n",
    "                    answer['url']=url\n",
    "                    answer['base']=s.search(url).group()\n",
    "                    answer['text']='No text returned'\n",
    "                    answer['title']=\"\"\n",
    "                    yield answer\n",
    "    else:\n",
    "        answer['text']='This is not a proper url'\n",
    "        answer['url']=url\n",
    "        answer['base']=''\n",
    "        answer['title']=\"\"\n",
    "        yield answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Function\n",
    "\n",
    "Here is a test.  The `done` dictionary is important; it keeps you from repeating calls to urls you've already processed.  It's like \"caching\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-17T12:47:12.249766",
     "start_time": "2017-07-17T12:47:01.969779Z"
    }
   },
   "outputs": [],
   "source": [
    "# create vectorized function\n",
    "vect = np.vectorize(textgetter)\n",
    "\n",
    "#vectorize the operation\n",
    "cc = vect(vegastimedfil['sourceurl'].values[10:25])\n",
    "\n",
    "#Vectorized opp\n",
    "dd = list(next(l) for  l in cc)\n",
    "\n",
    "# the output\n",
    "pd.DataFrame(dd).head(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:geotwitter]",
   "language": "python",
   "name": "conda-env-geotwitter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
